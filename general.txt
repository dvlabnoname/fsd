import numpy as np

class Adaline:
    def __init__(self, lr=0.01, iters=10):
        self.lr = lr
        self.iters = iters

    def fit(self, X, y):
        self.w = np.zeros(1 + X.shape[1])
        self.cost_ = []

        for _ in range(self.iters):
            output = np.dot(X, self.w[1:]) + self.w[0]
            error = y - output
            self.w[1:] += self.lr * X.T.dot(error)
            self.w[0] += self.lr * error.sum()
            cost = (error**2).sum() / 2.0
            self.cost_.append(cost)

    def predict(self, X):
        return np.dot(X, self.w[1:]) + self.w[0]

X = np.array([[1, 2], [2, 3], [3, 4]])
y = np.array([1.5, 2.5, 3.5]) 

X_std = (X - X.mean(axis=0)) / X.std(axis=0)

model = Adaline(iters=100)
model.fit(X, y)

print("Weights:", model.w)
print("Prediction (raw):", model.predict((np.array([6,8]) - X.mean(axis=0)) / X.std(axis=0)))





# Find-S algo
import csv

with open('trainingdata.csv') as f:
    data = [row for row in csv.reader(f) if row and row[-1].strip().lower() == 'yes']

hypothesis = data[0][:-1]  # Initialize with first positive example
for row in data[1:]:
    for i in range(len(hypothesis)):
        if hypothesis[i] != row[i]:
            hypothesis[i] = '?'

print("Most specific hypothesis:", hypothesis)

# # Format of your CSV file should be this for Find-S algo
# sky,airTemp,humidity,wind,water,forecast,enjoySport
# Sunny,Warm,Normal,Strong,Warm,Same,Yes
# Sunny,Warm,High,Strong,Warm,Same,Yes
# Rainy,Cold,High,Strong,Warm,Change,No
# Sunny,Warm,High,Strong,Cool,Change,Yes






#Candidate Elimination
import pandas as pd, numpy as np

data = pd.read_csv('2.csv')
X, y = np.array(data.iloc[:, :-1]), np.array(data.iloc[:, -1])
S = X[0].copy()
G = [["?" for _ in S] for _ in S]

for i, x in enumerate(X):
    if y[i] == 'yes':
        S = ["?" if S[j] != x[j] else S[j] for j in range(len(S))]
    else:
        for j in range(len(S)):
            if S[j] != x[j]: G[j][j] = S[j]
            else: G[j][j] = "?"

G = [g for g in G if g != ["?"] * len(S)]
print("Final Specific Hypothesis:\n", S)
print("Final General Hypotheses:\n", G)

# #CSV format for candidate elimination
# Sky,Temp,Humidity,Wind,Water,Forecast,EnjoySport
# Sunny,Warm,Normal,Strong,Warm,Same,yes
# Sunny,Warm,High,Strong,Warm,Same,yes
# Rainy,Cold,High,Strong,Warm,Change,no
# Sunny,Warm,High,Strong,Cool,Change,yes







#Naive Bayes

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data = pd.read_csv('your_file.csv')
X, y = data.iloc[:, :-1], data.iloc[:, -1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = GaussianNB().fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))









#Kmeans vs EM program no:8

from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

X = load_iris().data
kmeans = KMeans(n_clusters=3, random_state=42).fit(X)
gmm = GaussianMixture(n_components=3, random_state=42).fit(X)
k_labels, g_labels = kmeans.predict(X), gmm.predict(X)
print(f"k-Means Silhouette: {silhouette_score(X, k_labels):.2f}, GMM Silhouette: {silhouette_score(X, g_labels):.2f}")
plt.subplot(1,2,1); plt.scatter(X[:,0], X[:,1], c=k_labels); plt.title("k-Means")
plt.subplot(1,2,2); plt.scatter(X[:,0], X[:,1], c=g_labels); plt.title("GMM (EM)"); plt.show()







#KNN on iris dataset

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)
predictions = model.predict(X_test)

for i, (pred, actual) in enumerate(zip(predictions, y_test)):
    print(f"{i+1}. Predicted: {pred}, Actual: {actual}", "✅" if pred == actual else "❌")






#Locally weighted regression with synthetic data
import numpy as np
import matplotlib.pyplot as plt

def lwlr(x0,X,Y,tau):
    X_b=np.c_[np.ones(len(X)),X]; x0_b=np.r_[1,x0]
    w=np.exp(-np.sum((X_b - x0_b)**2,axis=1)/(2*tau**2))
    W=np.diag(w); beta=np.linalg.pinv(X_b.T@W@X_b)@X_b.T@W@Y
    return x0_b@beta

X=np.linspace(-3,3,200); Y=np.log(np.abs(X**2-1)+0.5)+np.random.normal(0,0.1,200)
domain=np.linspace(-3,3,300)

# for tau in [10,1,0.1,0.01]:
#     plt.plot(domain,[lwlr(x,X,Y,tau) for x in domain],label=f'tau={tau}')
tau = 0.1
plt.plot(domain,[lwlr(x,X,Y,tau) for x in domain],label=f'tau={tau}')
plt.scatter(X,Y,alpha=0.3); plt.legend(); plt.show()







#Locally weighted regression with dataset of our own choice

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def kernel(x0, X, tau): return np.exp(-np.sum((X - x0)**2, axis=1) / (2 * tau**2))
def lwr(x0, X, Y, tau):
    X_b = np.c_[np.ones(len(X)), X]; x0_b = np.r_[1, x0]
    W = np.diag(kernel(x0_b, X_b, tau))
    beta = np.linalg.pinv(X_b.T @ W @ X_b) @ X_b.T @ W @ Y
    return x0_b @ beta

data = pd.read_csv('your_file.csv') #Replace this with your file name which contains x and y coordibnates
X, Y = data['x'].values, data['y'].values
domain = np.linspace(X.min(), X.max(), 200)
preds = [lwr(x0, X, Y, tau=0.5) for x0 in domain]

plt.scatter(X, Y, alpha=0.5)
plt.plot(domain, preds, color='red')
plt.show()

# x,y
# -3.0,-0.1411
# -2.8,-0.3624
# -2.6,-0.6347
# -2.4,-0.7467
# -2.2,-0.8085
# -2.0,-0.9093
# -1.8,-0.9738
# -1.6,-0.9996
# -1.4,-0.9855
# -1.2,-0.9320
# -1.0,-0.8415
# -0.8,-0.7174
# -0.6,-0.5646
# -0.4,-0.3894
# -0.2,-0.1987
# 0.0,0.0000
# 0.2,0.1987
# 0.4,0.3894
# 0.6,0.5646
# 0.8,0.7174
# 1.0,0.8415
# 1.2,0.9320
# 1.4,0.9855
# 1.6,0.9996
# 1.8,0.9738
# 2.0,0.9093
# 2.2,0.8085
# 2.4,0.6755
# 2.6,0.5155
# 2.8,0.3349
# 3.0,0.1411



